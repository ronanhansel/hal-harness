import os
import json
import time
import shutil
import tempfile
from pathlib import Path
from typing import Dict, Any, List
import docker

from .base_benchmark import BaseBenchmark


class SciCodeBenchmark(BaseBenchmark):
    """SciCode benchmark implementation"""

    def __init__(self, agent_dir: str, config: Dict[str, Any], benchmark_name: str = 'scicode'):
        self.benchmark_name = benchmark_name
        self.requires_sandbox = False
        super().__init__(agent_dir, config, requires_sandbox=self.requires_sandbox)

        # Load the dataset - check for custom path first
        self.dataset = self._load_dataset(config)
        self.benchmark = {task['problem_id']: task for task in self.dataset}

        # Set benchmark directory.
        self.benchmark_dir = os.path.join(os.path.dirname(__file__), 'SciCode')

    def _load_dataset(self, config: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Load dataset from custom path or HuggingFace."""
        # Check environment variable first
        custom_path = os.environ.get("SCICODE_DATASET_PATH")
        print(f"[SciCode] SCICODE_DATASET_PATH env var: {custom_path}")

        # Also check config for dataset_path
        if not custom_path and config:
            custom_path = config.get("dataset_path")
            print(f"[SciCode] dataset_path from config: {custom_path}")

        if custom_path:
            exists = os.path.exists(custom_path)
            print(f"[SciCode] Custom path exists: {exists}")
            if exists:
                print(f"[SciCode] Loading dataset from custom path: {custom_path}")
                with open(custom_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
                print(f"[SciCode] Loaded {len(data)} tasks from custom dataset")
                return data

        # Default: load from HuggingFace
        print(f"[SciCode] Loading from HuggingFace (no custom path or path doesn't exist)")
        try:
            from datasets import load_dataset
        except ImportError as exc:
            raise ImportError(
                "datasets package is required to load SciCode from HuggingFace. "
                "Set SCICODE_DATASET_PATH to a local dataset to avoid this dependency."
            ) from exc
        data = list(load_dataset("SciCode1/SciCode", split="test"))
        print(f"[SciCode] Loaded {len(data)} tasks from HuggingFace")
        return data
        
    def evaluate_output(self, agent_output: Dict[str, Any], run_id: str) -> Dict[str, Any]:
        """Run SciCode evaluation harness on agent outputs"""
        start_time = time.time()
        # Create a temporary directory on the host for evaluation files.
        host_tmp_dir = Path(tempfile.mkdtemp(prefix=f"scicode_eval_{run_id}_"))

        if self.benchmark_name == 'scicode_hard':
            # Iterate through problems 
            for problem_id, code_content in agent_output.items():
                task = self.benchmark.get(problem_id)
                
                tmp_file = host_tmp_dir / f"{problem_id}.py"
                with open(tmp_file, "w", encoding="utf-8") as f:
                    f.write(code_content)
                    f.write("\n\nfrom scicode.parse.parse import process_hdf5_to_tuple\n")
                    last_step = task["sub_steps"][-1]
                    step_id = last_step["step_number"]
                    test_lst = last_step["test_cases"]
                    f.write(f"targets = process_hdf5_to_tuple('{step_id}', {len(test_lst)})\n")
                    for idx in range(len(test_lst)):
                        f.write(f"target = targets[{idx}]\n\n")
                        for line in test_lst[idx].split("\n"):
                            f.write(line + "\n")
            
        else:
            # Write evaluation files (one per agent_output entry).
            for problem_id, subtasks in agent_output.items():
                try:
                    items = subtasks.items()
                except Exception as e:
                    print(f"Error processing {problem_id}: {e}")
                    continue
                for subtask, code_content in subtasks.items():
                    parts = subtask.split(".")
                    subtask_step = parts[1]
                    task = self.benchmark.get(problem_id)
                    step_index = int(subtask_step) - 1
                    step_info = task["sub_steps"][step_index]
                    step_id = step_info["step_number"]
                    test_lst = step_info["test_cases"]
                    
                    tmp_file = host_tmp_dir / f"{problem_id}.{subtask_step}.py"

                    with open(tmp_file, "w", encoding="utf-8") as f:
                        f.write("import scipy.integrate\n")
                        f.write("if not hasattr(scipy.integrate, 'simps'): scipy.integrate.simps = scipy.integrate.simpson\n")
                        f.write("import numpy\n")
                        f.write("if not hasattr(numpy, 'trapz'): numpy.trapz = numpy.trapezoid\n")
                        f.write(code_content)
                        f.write("\n\nfrom scicode.parse.parse import process_hdf5_to_tuple\n")
                        f.write(f"targets = process_hdf5_to_tuple('{step_id}', {len(test_lst)})\n")
                        for idx in range(len(test_lst)):
                            f.write(f"target = targets[{idx}]\n\n")
                            for line in test_lst[idx].split("\n"):
                                f.write(line + "\n")
        
        image_name = os.environ.get("SCICODE_EVAL_IMAGE", "python:3.11")
        skip_install = os.environ.get("SCICODE_EVAL_SKIP_INSTALL", "0").lower() in {"1", "true", "yes"}

        # Launch a Docker container that mounts self.benchmark_dir at /app and host_tmp_dir at /app/tmp_eval.
        docker_timeout = int(os.environ.get("HAL_DOCKER_TIMEOUT", "600"))
        docker_host = os.environ.get("HAL_DOCKER_HOST") or os.environ.get("DOCKER_HOST")
        
        if docker_host:
            client = docker.DockerClient(base_url=docker_host, timeout=docker_timeout)
        else:
            client = docker.from_env(timeout=docker_timeout)

        container = client.containers.run(
            image_name,
            command="tail -f /dev/null",
            volumes={
                str(Path(self.benchmark_dir).resolve()): {"bind": "/app", "mode": "rw"},
                str(host_tmp_dir.resolve()): {"bind": "/app/tmp_eval", "mode": "rw"}
            },
            working_dir="/app",
            environment={"PYTHONPATH": "/app/src"},
            detach=True,
        )
        
        try:
            if skip_install:
                print("Skipping pip install inside container (SCICODE_EVAL_SKIP_INSTALL=1).")
            else:
                # Install required dependencies inside the container.
                print("Installing dependencies inside the container...")
                install_result = container.exec_run("pip install -e .", stream=True, demux=True)
                for stdout, stderr in install_result.output:
                    if stdout:
                        print(stdout.decode(), end='')
                    if stderr:
                        print(stderr.decode(), end='')

            # Helper function to execute a Python script inside the container.            
            def run_script(script_path: Path) -> int:
                # Compute the file's relative path in the host temporary directory and convert it
                rel_path = script_path.relative_to(host_tmp_dir)
                container_script_path = f"/app/tmp_eval/{rel_path}"
                
                # Run script within container with timeout exception
                cmd = ["timeout", "60", "python", container_script_path]
                try:
                    result = container.exec_run(cmd, demux=True)
                    if result.exit_code == 124:
                        print(f"Timeout running script [{script_path.name}] after 60 seconds")
                        return 2
                    elif result.exit_code == 0:
                        print(f"Successfully executed script: {script_path.name}")
                        return 0
                    else:
                        print(f"Error running script [{script_path.name}]: exit code {result.exit_code}")
                        stdout, stderr = result.output
                        error_message = stderr.decode("utf-8") if stderr and isinstance(stderr, bytes) else stderr
                        print(f"Error running script [{script_path.name}]: exit code {result.exit_code}")
                        if error_message:
                            print("Error message:", error_message)
                        return 1
                except Exception as e:
                    print(f"Exception running script [{script_path.name}]: {e}")
                    return 1
            
            # Initialize a dictionary to track outcomes.
            correct_dict = {problem_id: [] for problem_id in self.benchmark.keys()}
            
            # Execute each temporary file inside the container.
            for tmp_file in host_tmp_dir.iterdir():
                if tmp_file.is_file():
                    func_id = tmp_file.stem  # e.g., "11.3"
                    prob_id = func_id.split('.')[0]
                    ret = run_script(tmp_file)
                    if ret == 0:
                        correct_dict[prob_id].append(func_id)
            
            return {"details": correct_dict}
        
        finally:
            # Cleanup: stop and remove the container, then delete the temporary directory.
            container.stop()
            container.remove()
            shutil.rmtree(host_tmp_dir)
    
    def get_metrics(self, eval_results: Dict[str, Any]) -> Dict[str, Any]:
        """Extract metrics from evaluation results"""

        # Load results of evaluation
        details = eval_results.get("details", {})

        # Track relevant variables
        successful_tasks = []
        failed_tasks = []
        total_passed_subtasks = 0
        total_subtasks = 0

        # Iterate through benchmark tasks
        for problem_id, task in self.benchmark.items():
            # Get the number of subtasks in the benchmark
            total = len(task.get("sub_steps", []))
            # Get the number of subtasks that were successfully executed
            passed = len(details.get(problem_id, []))
            if self.benchmark_name == 'scicode_hard':
                # For the hard benchmark, we only care about the last step
                if passed > 0:
                    successful_tasks.append(problem_id)
                else:
                    failed_tasks.append(problem_id)
            else:
                # For the easy and standard benchmarks, we check whether all steps are passed
                total_subtasks += total
                total_passed_subtasks += passed
                if total > 0 and passed == total:
                    successful_tasks.append(problem_id)
                else:
                    failed_tasks.append(problem_id)

        accuracy = len(successful_tasks) / (len(successful_tasks) + len(failed_tasks))

        if self.benchmark_name != 'scicode_hard':
            subtask_accuracy = total_passed_subtasks / total_subtasks if total_subtasks > 0 else 0

            return {
                "accuracy": accuracy,
                "subtask_accuracy": subtask_accuracy,
                "successful_tasks": successful_tasks,
                "failed_tasks": failed_tasks
            }
        
        else:

            return {
                "accuracy": accuracy,
                "successful_tasks": successful_tasks,
                "failed_tasks": failed_tasks
            }
